{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADA Project : Milestone 2 Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will introduce you to the dataset that we chose by locally importing a part of in, and store it in a dataframe. Hence, we will be able to have an insight on the work that we will perform on the full dataset, once everything is set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from pyspark.sql import *\n",
    "from pyspark import SparkContext, SQLContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Twitter dataset data collection, from cluster to dataframe\n",
    "\n",
    "In this section, we will make some operation with the help of Spark, to access, filter and export the useful tweets from the cluster to our computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few words about what we noticed for our dataset \n",
    "\n",
    "First, the twitter dataset starts from year 2012.\n",
    "In the date section, the hour has been scaled, so that the tweet time is always relative to GMT+00. This will be of use when we will relate tweet dates and times with the Wikipedia dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the useful tweets\n",
    "\n",
    "We start by declaring the Spark Context in order to make the link with the cluster. With Spark installed locally, we are able to query the cluster directly in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_file = sc.textFile(\"hdfs:///datasets/tweets-leon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of our filter is that we want to work with data that is already highly focused on our subject : terrorist  attacks. Moreover, to avoid some bias that could appear from the selection of a subset of the entire Twitter database, we will consider all the most common languages, and select the interesting tweets independently of the language.\n",
    "Indeed, the filter is composed of lists of words of different weight, for each language. When a tweet is passed through the filter, we will compute the tweet score depending on it's content and, if the score is high enough, select the tweet to be part of our dataframe. Since all selected words are translated, this should harmonize the selection of our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We define below a few helper functions that will be used for our inition filter :** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first helper function is used to translate a word of interest in every selected language :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def translate_word(w,languages):\n",
    "    \n",
    "    translator = Translator()\n",
    "\n",
    "\n",
    "    w_dict = {}\n",
    "    \n",
    "    for l in languages:\n",
    "        t = translator.translate(w,dest=l)\n",
    "        w_dict[l]=t.text\n",
    "        \n",
    "    \n",
    "    \n",
    "    return w_dict\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is the heart of the filter. Languages and words are selected and inputed directly into three different lists of different weight. They are translated and put into three dicts, the keys being the language and the values being all words of the selected language, from the selected \"importance list\".\n",
    "The function outputs the three dicts, one for each importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def words_to_match():\n",
    "    \n",
    "    big_dict1 = {}\n",
    "    big_dict2 = {}\n",
    "    big_dict3 = {}\n",
    "    languages = ['en','fr','es','it','nl','de']\n",
    "    \n",
    "    for l in languages:\n",
    "        big_dict1[l] = []\n",
    "        big_dict2[l] = []\n",
    "        big_dict3[l] = []\n",
    "    \n",
    "    interesting_words_t1 = ['terrorism', 'terrorist', 'terror attack']\n",
    "    for w in interesting_words_t1:\n",
    "        d = translate_word(w,languages)\n",
    "        for (k,v) in d.items():\n",
    "            big_dict1[k].append(v)\n",
    "\n",
    "    interesting_words_t2 = ['isis', 'bomb','hostage']\n",
    "    for w in interesting_words_t2:\n",
    "        d = translate_word(w,languages)\n",
    "        for (k,v) in d.items():\n",
    "            big_dict2[k].append(v)\n",
    "\n",
    "\n",
    "    interesting_words_t3 = ['weapon','knife','assault rifle','dead','deaths','died','injured','kill','attentat']\n",
    "    for w in interesting_words_t3:\n",
    "        d = translate_word(w,languages)\n",
    "        for (k,v) in d.items():\n",
    "            big_dict3[k].append(v)\n",
    "        \n",
    "    #list_special_words = []\n",
    "    \n",
    "\n",
    "    return (big_dict1,big_dict2,big_dict3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function computes the importance of a tweet by assigning specific weights to every tweet. The assignment is done by iterating on all interesting words, looking whether they occur in the tweet content. According to the word's affiliation to one of the three dicts, different weight is incremented. If the total weight of the tweet reaches the threshold value (here 1.0), the filter returns True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_interesting(content,big_dicts):\n",
    "    \n",
    "\n",
    "    content = parse_tweet_content(content)\n",
    "    \n",
    "    languages = ['en','fr','es','it','nl','de'] \n",
    "\n",
    "    lang = content[:2]\n",
    "    \n",
    "    # Here we are concentrating our analysis in a limited number of languages\n",
    "    if lang not in languages:\n",
    "        return False\n",
    "    \n",
    "    #TODO\n",
    "    #no specific place, specific religion, specific ethnic group etc.\n",
    "    #exception : big terrorist groups ?\n",
    "    #inclure filtre ville\n",
    "    \n",
    "    weight=0.0\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for w in big_dicts[0][lang]:\n",
    "        if w in content:\n",
    "            weight+=1.0\n",
    "\n",
    "    for w in big_dicts[1][lang]:\n",
    "        if w in content:\n",
    "            weight+=0.5\n",
    "\n",
    "    for w in big_dicts[2][lang]:\n",
    "        if w in content:\n",
    "            weight+=0.3\n",
    "             \n",
    "    return (weight >= 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We declare the variable `bds` to be the three filtering dictionnaries. It will serve as an input of our filtering function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bds = words_to_match()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we actually call spark by filtering the data in the cluster with our filter, to then take a subset of defined size. We proceed to write it to a text file for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "terrorism = text_file.filter(lambda t: is_interesting(t,bds)).take(4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_t = open('tweets_terror2.txt','w')\n",
    "for item in terrorism:\n",
    "    file_t.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an important part of our analysis, we want to have a feel on the percentage of the tweets that we filter. According to our filter's characteristics, the ratio of tweets of interest over all tweets is of 1/3000. Taking a total tweet count of around 18 billion, the total tweets that are interesting for us is of 6 million."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terr = text_file.take(300000)\n",
    "\n",
    "count=0\n",
    "for t in terr:\n",
    "    if is_interesting(t,bds):\n",
    "        count+=1\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that we get almost 100 matches for a subset of 300'000 tweets, illustrating the ratio of 1/3000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fr\\t345963978092072960\\tSat Jun 15 18:00:14 +0000 2013\\tKazdaliMaaradj\\tPakistan: un double-attentat à la bombe à Quetta (sud-ouest) fait au moins 23 morts (nouveau bilan des autorités locales)',\n",
       " 'en\\t345964011382259712\\tSat Jun 15 18:00:22 +0000 2013\\tSumairaALi4\\t#BLA needs to be targetted in INDIA and UK. ISI should get in motion as were in 80s #JudicialTerrorism',\n",
       " 'en\\t345964045007978497\\tSat Jun 15 18:00:30 +0000 2013\\tMonotheist_\\tUnrest in Baluchistan. BLA terrorism there. Baluch demand justice. Foreign and local intelligence are involved. No one dares to anyone',\n",
       " 'es\\t345964057632837632\\tSat Jun 15 18:00:33 +0000 2013\\texodo3013\\t@akatsuky1000 quien ayudo a librar al pueblo de Libia del terrorista #1 en el mundo Omar K  que masacraba a su pueblo con aviones de guerra',\n",
       " 'es\\t345964070274482176\\tSat Jun 15 18:00:36 +0000 2013\\tCesar_Soto_16\\tCon Los Terroristas - Alianza Metal 1°H: http://t.co/SDT9qzSVHz vía @YouTube']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terrorism[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling the filtered tweets\n",
    "\n",
    "#### Some issues we encountered:\n",
    "\n",
    "1)    Tweets can countain retweet so many times the same tweet can appear with a retweet identification: `RT @<username>`\n",
    "    - Resolved by adding Frequency parameter for tweet that has been retweet \n",
    "    - Even tough we separeted the tweet from the retweet some of the tweets appears many time without the Retweet identification. It is still important to distinguish them and not count them many times since we reckon that simply copying a message or retweeting a message has less significance than creating it.\n",
    "    \n",
    "2)    Even if we remove the retweet, some tweets are still the same but have not the same length which can lead to count separetly the same tweet\n",
    "    - Resolved by putting a fixed max length to all tweet\n",
    "    - Or by testing if a string is in another (Complicated solution not adopted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the filtered tweets from the .txt files\n",
    "tweets_raw = pd.read_table(filepath_or_buffer='tweets_terror2.txt', names=[\"lan\",\"id\",\"date\", \"user_name\", \"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, the id and user name of the tweet is useless, we keep therefore only the language, the date and the content of the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_raw = tweets_raw.drop(axis= 1, labels=  [\"id\", \"user_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The date countained in the tweets has been translated into `GMT` 0. So we do not have to worry about translating the date and can directly standarize with the dateutil.parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We parse the date to have a uniform \n",
    "tweets_raw[\"date\"] = tweets_raw[\"date\"].apply(lambda d: parse(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = tweets_raw.copy()\n",
    "tweets[\"retweet\"] =  tweets[\"content\"].map(lambda s : s[0:4] == \"RT @\") #Is it a retweet?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we need to normalize our tweet to handle 1) and 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Maximum length that we allowed to have in oder to not have different tweet\n",
    "\n",
    "MAX_LEN = 140 - 15 - 10  # Limit of a tweet minus the maximum user name \n",
    "                         # and other charachter added when a retweet is created\n",
    "\n",
    "\n",
    "def remove_retweet_and_cut(t):\n",
    "    \"\"\"\n",
    "    Function that remove the RT @ in front of a tweet if it has been detected as a retweet, \n",
    "    And cut the tweet according to the MAX_LEN parameter\n",
    "    \"\"\"\n",
    "    \n",
    "    if(t[\"retweet\"]):\n",
    "        return ' '.join(t[\"content\"].split()[2:])[0:MAX_LEN]\n",
    "    else :\n",
    "        return t[\"content\"][0:MAX_LEN]\n",
    "    \n",
    "\n",
    "    \n",
    "#Apply the function we just created    \n",
    "tweets[\"content\"] =  tweets.apply(remove_retweet_and_cut, axis = 1)\n",
    "\n",
    "\n",
    "#------------------------- Handling the frequency of a tweet ---------------------\n",
    "\n",
    "\n",
    "# We create a dict to map the content and the frequency that a tweet with the same content occur.\n",
    "freq_dict = dict(tweets.groupby(\"content\")[\"lan\"].count())\n",
    "\n",
    "\n",
    "tweets = tweets.drop_duplicates(subset=\"content\")\n",
    "\n",
    "\n",
    "tweets[\"frequency\"] = tweets[\"content\"].map(lambda c : freq_dict[c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We end up with a nice dataframe of the filtered tweets with the frequency of each tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lan</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>retweet</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2153</th>\n",
       "      <td>en</td>\n",
       "      <td>2013-08-02 12:00:53+00:00</td>\n",
       "      <td>Zayn is NOT a terrorist.\\nZayn donated for cha...</td>\n",
       "      <td>True</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>en</td>\n",
       "      <td>2013-07-16 04:00:12+00:00</td>\n",
       "      <td>The whites agree to stop blaming all Arab's fo...</td>\n",
       "      <td>True</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>en</td>\n",
       "      <td>2013-06-15 18:01:53+00:00</td>\n",
       "      <td>Black Crime =Gang Violence. \\nArab Crime = Ter...</td>\n",
       "      <td>True</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3596</th>\n",
       "      <td>fr</td>\n",
       "      <td>2013-07-08 10:40:40+00:00</td>\n",
       "      <td>RTsi arabe a la piscine :\\n-Jvais faire la bom...</td>\n",
       "      <td>True</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3257</th>\n",
       "      <td>es</td>\n",
       "      <td>2012-10-06 14:00:21+00:00</td>\n",
       "      <td>Cuba demanda justicia en el Día de las Víctima...</td>\n",
       "      <td>False</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lan                      date  \\\n",
       "2153  en 2013-08-02 12:00:53+00:00   \n",
       "769   en 2013-07-16 04:00:12+00:00   \n",
       "8     en 2013-06-15 18:01:53+00:00   \n",
       "3596  fr 2013-07-08 10:40:40+00:00   \n",
       "3257  es 2012-10-06 14:00:21+00:00   \n",
       "\n",
       "                                                content  retweet  frequency  \n",
       "2153  Zayn is NOT a terrorist.\\nZayn donated for cha...     True         37  \n",
       "769   The whites agree to stop blaming all Arab's fo...     True         25  \n",
       "8     Black Crime =Gang Violence. \\nArab Crime = Ter...     True         19  \n",
       "3596  RTsi arabe a la piscine :\\n-Jvais faire la bom...     True         16  \n",
       "3257  Cuba demanda justicia en el Día de las Víctima...    False         15  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.sort_values(by=\"frequency\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lan</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>retweet</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fr</td>\n",
       "      <td>2013-06-15 18:00:14+00:00</td>\n",
       "      <td>Pakistan: un double-attentat à la bombe à Quet...</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2609</th>\n",
       "      <td>en</td>\n",
       "      <td>2013-08-02 12:29:20+00:00</td>\n",
       "      <td>@RuckaRuckaAli I love reading how these holy #...</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2610</th>\n",
       "      <td>es</td>\n",
       "      <td>2013-08-02 12:29:21+00:00</td>\n",
       "      <td>#UnDíaComoHoy pero de 1980: en la estación fer...</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2612</th>\n",
       "      <td>en</td>\n",
       "      <td>2013-08-02 12:29:24+00:00</td>\n",
       "      <td>@Harry_Styles I love you. Please follow me my ...</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2614</th>\n",
       "      <td>en</td>\n",
       "      <td>2013-08-02 12:29:27+00:00</td>\n",
       "      <td>@NancyAtwal: You call him a terrorist I call h...</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lan                      date  \\\n",
       "0     fr 2013-06-15 18:00:14+00:00   \n",
       "2609  en 2013-08-02 12:29:20+00:00   \n",
       "2610  es 2013-08-02 12:29:21+00:00   \n",
       "2612  en 2013-08-02 12:29:24+00:00   \n",
       "2614  en 2013-08-02 12:29:27+00:00   \n",
       "\n",
       "                                                content  retweet  frequency  \n",
       "0     Pakistan: un double-attentat à la bombe à Quet...    False          1  \n",
       "2609  @RuckaRuckaAli I love reading how these holy #...    False          1  \n",
       "2610  #UnDíaComoHoy pero de 1980: en la estación fer...    False          1  \n",
       "2612  @Harry_Styles I love you. Please follow me my ...    False          1  \n",
       "2614  @NancyAtwal: You call him a terrorist I call h...    False          1  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here are the single tweets\n",
    "tweets.sort_values(by=\"frequency\", ascending=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see below that the ratio of retweet is consequent. \n",
    "Indeed, roughly 1/3 of our filtered tweets have been retweeted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3538555318500457"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[\"retweet\"].sum()/len(tweets.retweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grp_tweet = tweets.groupby(\"lan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lan\n",
       "en    2388\n",
       "es     688\n",
       "fr      96\n",
       "it      79\n",
       "nl      30\n",
       "Name: content, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grp_tweet[\"content\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that, not surprisingly, we have more english tweets than the other languages. Indeed english is the most common widespread language and spanish the second one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data from Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we scrape data from Wikipedia. We want to access the tables that register the terror attacks that happened at some point in the past. There are some Wikipedia articles (such as https://en.wikipedia.org/wiki/List_of_terrorist_incidents_in_January-June_2011) that do exactly that. The data is presented as tables, and all the articles that we need present data in this form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simple map of month name to its number\n",
    "month_to_int = {\n",
    "    'January': 1,\n",
    "    'February': 2,\n",
    "    'March': 3,\n",
    "    'April': 4,\n",
    "    'May': 5,\n",
    "    'June': 6,\n",
    "    'July': 7,\n",
    "    'August': 8,\n",
    "    'September': 9,\n",
    "    'October': 10,\n",
    "    'November': 11,\n",
    "    'December': 12\n",
    "}\n",
    "\n",
    "# Reversed map\n",
    "int_to_month = {i: m for m, i in month_to_int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The wikipedia URL that every article has in common\n",
    "base_url = 'https://en.wikipedia.org/wiki/List_of_terrorist_incidents_in_'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show all the articles that we are going to use to find the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['January-June_2011',\n",
       " 'July-December_2011',\n",
       " 'January-June_2012',\n",
       " 'July-December_2012',\n",
       " 'January-June_2013',\n",
       " 'July-December_2013',\n",
       " 'January-June_2014',\n",
       " 'July-December_2014',\n",
       " 'January_2015',\n",
       " 'February_2015',\n",
       " 'March_2015',\n",
       " 'April_2015',\n",
       " 'May_2015',\n",
       " 'June_2015',\n",
       " 'July_2015',\n",
       " 'August_2015',\n",
       " 'September_2015',\n",
       " 'October_2015',\n",
       " 'November_2015',\n",
       " 'December_2015',\n",
       " 'January_2016',\n",
       " 'February_2016',\n",
       " 'March_2016',\n",
       " 'April_2016',\n",
       " 'May_2016',\n",
       " 'June_2016',\n",
       " 'July_2016',\n",
       " 'August_2016',\n",
       " 'September_2016',\n",
       " 'October_2016',\n",
       " 'November_2016',\n",
       " 'December_2016',\n",
       " 'January_2017',\n",
       " 'February_2017',\n",
       " 'March_2017',\n",
       " 'April_2017',\n",
       " 'May_2017',\n",
       " 'June_2017',\n",
       " 'July_2017',\n",
       " 'August_2017',\n",
       " 'September_2017',\n",
       " 'October_2017',\n",
       " 'November_2017',\n",
       " 'December_2017']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All specific end of the wikipedia URL, along with the corresponding month numbers of the article\n",
    "times = {}\n",
    "\n",
    "for year in range(2011, 2015):\n",
    "    # For years 2011 to 2014, the articles appear biyearly\n",
    "    times.update({'January-June_' + str(year): list(range(1, 7))})\n",
    "    times.update({'July-December_' + str(year): list(range(7, 13))})\n",
    "    \n",
    "for year in range(2015, 2018):\n",
    "    # For years 2015 to 2017, the articles appear monthly\n",
    "    for month, int_ in month_to_int.items():\n",
    "        times.update({month + '_' + str(year): [int_]})\n",
    "        \n",
    "list(times.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_int(s):\n",
    "    '''Returns the first integer found in s'''\n",
    "    i = re.findall('\\d+', s)\n",
    "    return int(i[0]) if len(i) > 0 else float('NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_date(s, year):\n",
    "    '''Returns a date from the datetime library from a string like \\'January 1\\''''\n",
    "    l = s.split(' ')\n",
    "    return date(to_int(year), month_to_int[l[0]], to_int(l[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wiki_table_to_df(end_url, month_range, base_url=base_url):\n",
    "    '''Creates a dataframe from the tables available in the wikipedia page'''\n",
    "    print('Scraping for', end_url)\n",
    "    r = requests.get(base_url + end_url) # Get request\n",
    "    soup = BeautifulSoup(r.text, 'lxml') # Parse HTML\n",
    "    wiki_tables = soup.findAll('table', {'class': 'wikitable sortable'}) # Get tables from the wikipedia page\n",
    "\n",
    "    table = []\n",
    "\n",
    "    for month_int, wiki_table in zip(month_range, wiki_tables):\n",
    "        for row in wiki_table.findAll('tr'):\n",
    "            elems = row.findAll('td') \n",
    "            if len(elems) != 0:\n",
    "                interesting = [elem.text for elem in elems[:5]]\n",
    "                 # First element is the day of the month, but we add the name of the month as well in front of it\n",
    "                interesting[0] = int_to_month[month_int] + ' ' + interesting[0]\n",
    "                table.append(interesting)\n",
    "                \n",
    "    df = pd.DataFrame(table, columns=['date', 'type', 'deaths', 'injuries', 'location'])\n",
    "    df.date = df.date.apply(lambda s: to_date(s, end_url[-4:])) # Translate the date with the year defined by the end_url arg\n",
    "    df.deaths = df.deaths.apply(to_int) # Map death number to int\n",
    "    df.injuries = df.injuries.apply(to_int) # Map injuries number to int\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping for January-June_2011\n",
      "Scraping for July-December_2011\n",
      "Scraping for January-June_2012\n",
      "Scraping for July-December_2012\n",
      "Scraping for January-June_2013\n",
      "Scraping for July-December_2013\n",
      "Scraping for January-June_2014\n",
      "Scraping for July-December_2014\n",
      "Scraping for January_2015\n",
      "Scraping for February_2015\n",
      "Scraping for March_2015\n",
      "Scraping for April_2015\n",
      "Scraping for May_2015\n",
      "Scraping for June_2015\n",
      "Scraping for July_2015\n",
      "Scraping for August_2015\n",
      "Scraping for September_2015\n",
      "Scraping for October_2015\n",
      "Scraping for November_2015\n",
      "Scraping for December_2015\n",
      "Scraping for January_2016\n",
      "Scraping for February_2016\n",
      "Scraping for March_2016\n",
      "Scraping for April_2016\n",
      "Scraping for May_2016\n",
      "Scraping for June_2016\n",
      "Scraping for July_2016\n",
      "Scraping for August_2016\n",
      "Scraping for September_2016\n",
      "Scraping for October_2016\n",
      "Scraping for November_2016\n",
      "Scraping for December_2016\n",
      "Scraping for January_2017\n",
      "Scraping for February_2017\n",
      "Scraping for March_2017\n",
      "Scraping for April_2017\n",
      "Scraping for May_2017\n",
      "Scraping for June_2017\n",
      "Scraping for July_2017\n",
      "Scraping for August_2017\n",
      "Scraping for September_2017\n",
      "Scraping for October_2017\n",
      "Scraping for November_2017\n",
      "Scraping for December_2017\n",
      "We have 4664 registered attacks from January 1st, 2011 up to today (November 28th, 2017)\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "\n",
    "# Get a DataFrame for every article from 2011 to 2017\n",
    "for time, month_range in times.items():\n",
    "    dfs.append(wiki_table_to_df(time, month_range))\n",
    "    \n",
    "df = pd.concat(dfs)\n",
    "print('We have {} registered attacks from January 1st, 2011 up to today (November 28th, 2017)'.format(df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what some of the entries of the final result look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>date</th>\n",
       "      <th>type</th>\n",
       "      <th>deaths</th>\n",
       "      <th>injuries</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>Suicide bombing</td>\n",
       "      <td>21.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>Alexandria, Egypt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>56</td>\n",
       "      <td>2011-02-13</td>\n",
       "      <td>Raid</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Zamboanga, Philippines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1033</th>\n",
       "      <td>37</td>\n",
       "      <td>2014-11-18</td>\n",
       "      <td>Shooting, Melee attack</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Jerusalem, Israel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4663</th>\n",
       "      <td>41</td>\n",
       "      <td>2017-11-28</td>\n",
       "      <td>Bombing</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kandahar province, Afghanistan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index        date                    type  deaths  injuries  \\\n",
       "0         0  2011-01-01         Suicide bombing    21.0      97.0   \n",
       "56       56  2011-02-13                    Raid     7.0       5.0   \n",
       "1033     37  2014-11-18  Shooting, Melee attack     5.0       7.0   \n",
       "4663     41  2017-11-28                 Bombing     8.0       NaN   \n",
       "\n",
       "                            location  \n",
       "0                  Alexandria, Egypt  \n",
       "56            Zamboanga, Philippines  \n",
       "1033               Jerusalem, Israel  \n",
       "4663  Kandahar province, Afghanistan  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[[0, 56, 1033, -1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reindex and save\n",
    "df.to_csv('attacks.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
