{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADA Project : Milestone 2 Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will introduce you to the dataset that we chose by locally importing a part of in, and store it in a dataframe. Hence, we will be able to have an insight on the work that we will perform on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from pyspark.sql import *\n",
    "from pyspark import SparkContext, SQLContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Twitter dataset data collection, from cluster to dataframe\n",
    "\n",
    "In this section, we will make some operation with the help of Spark, to access, filter and export the useful tweets from the cluster to our computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few words about what we noticed for our dataset \n",
    "\n",
    "First, the twitter dataset starts from year 2012.\n",
    "In the date section, the hour has been scaled, so that the tweet time is always relative to GMT+00. This will be of use when we will relate tweet dates and times with the Wikipedia dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Filtering the useful tweets\n",
    "\n",
    "We start by declaring the Spark Context in order to make the link with the cluster. With Spark installed locally, we are able to query the cluster directly in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_file = sc.textFile(\"hdfs:///datasets/tweets-leon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of our filter is that we want to work with data that is already highly focused on our subject : terrorist  attacks. For the Milestone 2, we implemented a filter that considered different languages. Following the feedback from the TAs, we decided to stick with only English as the language of the tweets and the keywords, to go back to a **more simple, but more precise filter**. When a tweet is passed through the filter, we will compute the tweet score depending on its content and, if the score is high enough, select the tweet to be part of our dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We define below a few helper functions that will be used for our inition filter :** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is the heart of the filter. Five lists are detailed, representing words of different importance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def words_to_match():\n",
    "    \n",
    "\n",
    "    language = 'en'\n",
    "    \n",
    "    t1 = ['terror attack', 'terrorist attack','suicide bombing','mass shooting']\n",
    "\n",
    "\n",
    "\n",
    "    t2 = ['suicide bomber','car bombing','drone bombing','mass execution','improvised explosive device','truck bomb','grenade attack','train bombing']\n",
    "\n",
    "\n",
    "    t3 = [' ied', 'hijacking','genocide','bomb attack','vehicule attack','assasination','terrorism','weapon','knife','assault rifle','dead','deaths','died','injured','kill','plant','drive-by shooting','hostage','execution']\n",
    "\n",
    "\n",
    "    hashtag = ['#prayfor','#terrorism','#terrorists','#terrorattack']\n",
    "\n",
    "    malus_list = ['years ago','year ago', 'months ago','month ago','anniversary']\n",
    "    \n",
    "    l = [t1,t2,t3,hashtag,malus_list]\n",
    "    \n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function computes the importance of a tweet by assigning specific weights to every tweet. The assignment is done by iterating on all interesting words, looking whether they occur in the tweet content. According to the word's affiliation to one of the lists, different weight are incremented. If the total weight of the tweet reaches the threshold value (here 1.0), the filter returns True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_interesting(content,l):\n",
    "    \n",
    "    content = content.lower()\n",
    "    \n",
    "    lang = content[:2]\n",
    "    \n",
    "    \n",
    "    weight=0.0\n",
    "    \n",
    "    \n",
    "    \n",
    "    for w in l[0]:\n",
    "        if w in content:\n",
    "            weight+=1.0\n",
    "\n",
    "    for w in l[1]:\n",
    "        if w in content:\n",
    "            weight+=0.9\n",
    "\n",
    "    for w in l[2]:\n",
    "        if w in content:\n",
    "            weight+=0.1\n",
    "             \n",
    "    for w in l[3]:\n",
    "        if w in content:\n",
    "            weight+=0.7\n",
    "            \n",
    "            \n",
    "    for w in l[3]:\n",
    "        if w in content:\n",
    "            weight-=0.5\n",
    "    \n",
    "    return (weight >= 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We declare the variable `bds` to be the three filtering dictionnaries. It will serve as an input of our filtering function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bds = words_to_match()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we actually call spark by filtering the data in the cluster with our filter, to then take a subset of defined size. We proceed to write it to a text file for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "terrorism = text_file.filter(lambda t: is_interesting(t,bds)).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_t = open('tweets_terror3.txt','w')\n",
    "for item in terrorism:\n",
    "    file_t.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fr\\t345963978092072960\\tSat Jun 15 18:00:14 +0000 2013\\tKazdaliMaaradj\\tPakistan: un double-attentat à la bombe à Quetta (sud-ouest) fait au moins 23 morts (nouveau bilan des autorités locales)',\n",
       " 'en\\t345964011382259712\\tSat Jun 15 18:00:22 +0000 2013\\tSumairaALi4\\t#BLA needs to be targetted in INDIA and UK. ISI should get in motion as were in 80s #JudicialTerrorism',\n",
       " 'en\\t345964045007978497\\tSat Jun 15 18:00:30 +0000 2013\\tMonotheist_\\tUnrest in Baluchistan. BLA terrorism there. Baluch demand justice. Foreign and local intelligence are involved. No one dares to anyone',\n",
       " 'es\\t345964057632837632\\tSat Jun 15 18:00:33 +0000 2013\\texodo3013\\t@akatsuky1000 quien ayudo a librar al pueblo de Libia del terrorista #1 en el mundo Omar K  que masacraba a su pueblo con aviones de guerra',\n",
       " 'es\\t345964070274482176\\tSat Jun 15 18:00:36 +0000 2013\\tCesar_Soto_16\\tCon Los Terroristas - Alianza Metal 1°H: http://t.co/SDT9qzSVHz vía @YouTube']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terrorism[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Handling the filtered tweets\n",
    "\n",
    "#### Some issues we encountered:\n",
    "\n",
    "1)    Tweets can countain retweet so many times the same tweet can appear with a retweet identification: `RT @<username>`\n",
    "    - Resolved by adding Frequency parameter for tweet that has been retweet \n",
    "    - Even tough we separeted the tweet from the retweet some of the tweets appears many time without the Retweet identification. It is still important to distinguish them and not count them many times since we reckon that simply copying a message or retweeting a message has less significance than creating it.\n",
    "    \n",
    "2)    Even if we remove the retweet, some tweets are still the same but have not the same length which can lead to count separetly the same tweet\n",
    "    - Resolved by putting a fixed max length to all tweet\n",
    "    - Or by testing if a string is in another (Complicated solution not adopted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dateutil.parser import parse\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the filtered tweets from the .txt files\n",
    "tweets_raw = pd.read_csv(delimiter=\"\\t\",filepath_or_buffer='tweets_terr.txt', names=[\"lan\",\"id\",\"date\", \"user_name\", \"content\"],encoding='utf-8',quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lan</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>user_name</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en</td>\n",
       "      <td>3.459658e+17</td>\n",
       "      <td>Sat Jun 15 18:07:17 +0000 2013</td>\n",
       "      <td>SangyeH</td>\n",
       "      <td>RT @AnnieSage: Unbelievable.... @thinkprogress...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en</td>\n",
       "      <td>3.459658e+17</td>\n",
       "      <td>Sat Jun 15 18:07:27 +0000 2013</td>\n",
       "      <td>SR_Brant</td>\n",
       "      <td>RT @AnnieSage: Unbelievable.... @thinkprogress...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en</td>\n",
       "      <td>3.459683e+17</td>\n",
       "      <td>Sat Jun 15 18:17:35 +0000 2013</td>\n",
       "      <td>kiraababee</td>\n",
       "      <td>RT @SweaterGawd: I cum faster than the fbi dur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en</td>\n",
       "      <td>3.459687e+17</td>\n",
       "      <td>Sat Jun 15 18:19:07 +0000 2013</td>\n",
       "      <td>drgauravn85</td>\n",
       "      <td>@asma_rehman02 even your feeder USA is agreed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en</td>\n",
       "      <td>3.459700e+17</td>\n",
       "      <td>Sat Jun 15 18:24:06 +0000 2013</td>\n",
       "      <td>WatchTVChannels</td>\n",
       "      <td>Quetta Carnage: 23 killed in terrorist attacks...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lan            id                            date        user_name  \\\n",
       "0  en  3.459658e+17  Sat Jun 15 18:07:17 +0000 2013          SangyeH   \n",
       "1  en  3.459658e+17  Sat Jun 15 18:07:27 +0000 2013         SR_Brant   \n",
       "2  en  3.459683e+17  Sat Jun 15 18:17:35 +0000 2013       kiraababee   \n",
       "3  en  3.459687e+17  Sat Jun 15 18:19:07 +0000 2013      drgauravn85   \n",
       "4  en  3.459700e+17  Sat Jun 15 18:24:06 +0000 2013  WatchTVChannels   \n",
       "\n",
       "                                             content  \n",
       "0  RT @AnnieSage: Unbelievable.... @thinkprogress...  \n",
       "1  RT @AnnieSage: Unbelievable.... @thinkprogress...  \n",
       "2  RT @SweaterGawd: I cum faster than the fbi dur...  \n",
       "3  @asma_rehman02 even your feeder USA is agreed ...  \n",
       "4  Quetta Carnage: 23 killed in terrorist attacks...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_raw.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, the id and user name of the tweet is useless, we keep therefore only the language, the date and the content of the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_raw = tweets_raw.drop(axis= 1, labels=  [\"id\", \"user_name\"])\n",
    "tweets_raw = tweets_raw.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The date countained in the tweets has been translated into `GMT` 0. So we do not have to worry about translating the date and can directly standarize with the dateutil.parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We parse the date to have a uniform \n",
    "tweets_raw[\"date\"] = tweets_raw[\"date\"].apply(lambda d: parse(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = tweets_raw.copy()\n",
    "tweets[\"retweet\"] =  tweets[\"content\"].map(lambda s : s[0:4] == \"RT @\") #Is it a retweet?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we need to normalize our tweet to handle 1) and 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_http(t):\n",
    "    content = t.split()\n",
    "    for w in content:\n",
    "        \n",
    "        if \"http\" in w:\n",
    "            content.remove(w)\n",
    "    return \" \".join(content)\n",
    "\n",
    "\n",
    "\n",
    "# Maximum length that we allowed to have in oder to not have different tweet\n",
    "\n",
    "MAX_LEN = 140 - 15 - 10  # Limit of a tweet minus the maximum user name \n",
    "                         # and other charachter added when a retweet is created\n",
    "\n",
    "\n",
    "def remove_retweet_and_cut(t):\n",
    "    \"\"\"\n",
    "    Function that remove the RT @ in front of a tweet if it has been detected as a retweet, \n",
    "    And cut the tweet according to the MAX_LEN parameter\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if(t[\"retweet\"]):\n",
    "        return ' '.join(t[\"content\"].split()[2:])[0:MAX_LEN]\n",
    "    else :\n",
    "        return t[\"content\"][0:MAX_LEN]\n",
    "    \n",
    "\n",
    "    \n",
    "#Apply the functions we just created\n",
    "tweets[\"content\"] = tweets[\"content\"].map(remove_http)\n",
    "tweets[\"content\"] =  tweets.apply(remove_retweet_and_cut, axis = 1)\n",
    "\n",
    "\n",
    "#------------------------- Handling the frequency of a tweet ---------------------\n",
    "\n",
    "\n",
    "# We create a dict to map the content and the frequency that a tweet with the same content occur.\n",
    "freq_dict = dict(tweets.groupby(\"content\")[\"lan\"].count())\n",
    "\n",
    "\n",
    "tweets = tweets.drop_duplicates(subset=\"content\")\n",
    "\n",
    "\n",
    "tweets[\"frequency\"] = tweets[\"content\"].map(lambda c : freq_dict[c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We end up with a nice dataframe of the filtered tweets with the frequency of each tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lan</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>retweet</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>423016</th>\n",
       "      <td>en</td>\n",
       "      <td>2015-01-12 00:36:17+00:00</td>\n",
       "      <td>2,000 civilians killed in a terrorist attack i...</td>\n",
       "      <td>True</td>\n",
       "      <td>3006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>en</td>\n",
       "      <td>2012-12-17 03:29:35+00:00</td>\n",
       "      <td>Here they are - all SIXTY-ONE mass shootings i...</td>\n",
       "      <td>True</td>\n",
       "      <td>1585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361528</th>\n",
       "      <td>en</td>\n",
       "      <td>2014-12-17 20:14:55+00:00</td>\n",
       "      <td>Just heard about the terrorist attack, my hear...</td>\n",
       "      <td>True</td>\n",
       "      <td>1330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418277</th>\n",
       "      <td>en</td>\n",
       "      <td>2015-01-11 14:01:14+00:00</td>\n",
       "      <td>Chanting &amp;amp; applause in Paris as huge crowd...</td>\n",
       "      <td>True</td>\n",
       "      <td>1242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256112</th>\n",
       "      <td>en</td>\n",
       "      <td>2013-09-25 17:17:29+00:00</td>\n",
       "      <td>In 1962, the US government planned terrorist a...</td>\n",
       "      <td>True</td>\n",
       "      <td>1218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       lan                      date  \\\n",
       "423016  en 2015-01-12 00:36:17+00:00   \n",
       "352     en 2012-12-17 03:29:35+00:00   \n",
       "361528  en 2014-12-17 20:14:55+00:00   \n",
       "418277  en 2015-01-11 14:01:14+00:00   \n",
       "256112  en 2013-09-25 17:17:29+00:00   \n",
       "\n",
       "                                                  content  retweet  frequency  \n",
       "423016  2,000 civilians killed in a terrorist attack i...     True       3006  \n",
       "352     Here they are - all SIXTY-ONE mass shootings i...     True       1585  \n",
       "361528  Just heard about the terrorist attack, my hear...     True       1330  \n",
       "418277  Chanting &amp; applause in Paris as huge crowd...     True       1242  \n",
       "256112  In 1962, the US government planned terrorist a...     True       1218  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.sort_values(by=\"frequency\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lan</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>retweet</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>548846</th>\n",
       "      <td>en</td>\n",
       "      <td>2016-01-31 23:34:22+00:00</td>\n",
       "      <td>Where is the West's compassion &amp;amp; condemnat...</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291143</th>\n",
       "      <td>en</td>\n",
       "      <td>2013-12-29 09:23:08+00:00</td>\n",
       "      <td>#journals #news - Catherine Ashton's strong co...</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291144</th>\n",
       "      <td>en</td>\n",
       "      <td>2013-12-29 09:23:11+00:00</td>\n",
       "      <td>#journals #news - Catherine Ashton's strong co...</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291145</th>\n",
       "      <td>en</td>\n",
       "      <td>2013-12-29 09:23:21+00:00</td>\n",
       "      <td>#journals #news - Catherine Ashton's strong co...</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291146</th>\n",
       "      <td>en</td>\n",
       "      <td>2013-12-29 09:24:40+00:00</td>\n",
       "      <td>Catherine Ashton's strong condemnation of terr...</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       lan                      date  \\\n",
       "548846  en 2016-01-31 23:34:22+00:00   \n",
       "291143  en 2013-12-29 09:23:08+00:00   \n",
       "291144  en 2013-12-29 09:23:11+00:00   \n",
       "291145  en 2013-12-29 09:23:21+00:00   \n",
       "291146  en 2013-12-29 09:24:40+00:00   \n",
       "\n",
       "                                                  content  retweet  frequency  \n",
       "548846  Where is the West's compassion &amp; condemnat...    False          1  \n",
       "291143  #journals #news - Catherine Ashton's strong co...    False          1  \n",
       "291144  #journals #news - Catherine Ashton's strong co...    False          1  \n",
       "291145  #journals #news - Catherine Ashton's strong co...    False          1  \n",
       "291146  Catherine Ashton's strong condemnation of terr...    False          1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here are the single tweets\n",
    "tweets.sort_values(by=\"frequency\", ascending=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see below that the ratio of retweet is consequent. \n",
    "Indeed, roughly 1/3 of our filtered tweets have been retweeted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30570021327850488"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[\"retweet\"].sum()/len(tweets.retweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grp_tweet = tweets.groupby(\"lan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lan\n",
       "de      1028\n",
       "en    320770\n",
       "es      1045\n",
       "fr       699\n",
       "it       850\n",
       "nl      1473\n",
       "Name: content, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grp_tweet[\"content\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that, not surprisingly, we have more english tweets than the other languages. Indeed english is the most common widespread language and spanish the second one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data from Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we scrape data from Wikipedia. We want to access the tables that register the terror attacks that happened at some point in the past. There are some Wikipedia articles (such as https://en.wikipedia.org/wiki/List_of_terrorist_incidents_in_January-June_2011) that do exactly that. The data is presented as tables, and all the articles that we need present data in this form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simple map of month name to its number\n",
    "month_to_int = {\n",
    "    'January': 1,\n",
    "    'February': 2,\n",
    "    'March': 3,\n",
    "    'April': 4,\n",
    "    'May': 5,\n",
    "    'June': 6,\n",
    "    'July': 7,\n",
    "    'August': 8,\n",
    "    'September': 9,\n",
    "    'October': 10,\n",
    "    'November': 11,\n",
    "    'December': 12\n",
    "}\n",
    "\n",
    "# Reversed map\n",
    "int_to_month = {i: m for m, i in month_to_int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The wikipedia URL that every article has in common\n",
    "base_url = 'https://en.wikipedia.org/wiki/List_of_terrorist_incidents_in_'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show all the articles that we are going to use to find the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['July-December_2013',\n",
       " 'July-December_2011',\n",
       " 'January_2015',\n",
       " 'January-June_2014',\n",
       " 'July_2017',\n",
       " 'November_2017',\n",
       " 'June_2016',\n",
       " 'May_2017',\n",
       " 'January_2017',\n",
       " 'August_2015',\n",
       " 'May_2015',\n",
       " 'June_2017',\n",
       " 'December_2017',\n",
       " 'September_2017',\n",
       " 'September_2016',\n",
       " 'August_2017',\n",
       " 'July-December_2014',\n",
       " 'March_2016',\n",
       " 'March_2017',\n",
       " 'April_2015',\n",
       " 'January-June_2013',\n",
       " 'January-June_2011',\n",
       " 'April_2017',\n",
       " 'January_2016',\n",
       " 'October_2017',\n",
       " 'March_2015',\n",
       " 'February_2015',\n",
       " 'June_2015',\n",
       " 'October_2015',\n",
       " 'July_2016',\n",
       " 'November_2015',\n",
       " 'February_2016',\n",
       " 'July_2015',\n",
       " 'October_2016',\n",
       " 'May_2016',\n",
       " 'January-June_2012',\n",
       " 'February_2017',\n",
       " 'December_2015',\n",
       " 'August_2016',\n",
       " 'December_2016',\n",
       " 'April_2016',\n",
       " 'July-December_2012',\n",
       " 'November_2016',\n",
       " 'September_2015']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All specific end of the wikipedia URL, along with the corresponding month numbers of the article\n",
    "times = {}\n",
    "\n",
    "for year in range(2011, 2015):\n",
    "    # For years 2011 to 2014, the articles appear biyearly\n",
    "    times.update({'January-June_' + str(year): list(range(1, 7))})\n",
    "    times.update({'July-December_' + str(year): list(range(7, 13))})\n",
    "    \n",
    "for year in range(2015, 2018):\n",
    "    # For years 2015 to 2017, the articles appear monthly\n",
    "    for month, int_ in month_to_int.items():\n",
    "        times.update({month + '_' + str(year): [int_]})\n",
    "        \n",
    "list(times.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_int(s):\n",
    "    '''Returns the first integer found in s'''\n",
    "    i = re.findall('\\d+', s)\n",
    "    return int(i[0]) if len(i) > 0 else float('NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_date(s, year):\n",
    "    '''Returns a date from the datetime library from a string like \\'January 1\\''''\n",
    "    l = s.split(' ')\n",
    "    return date(to_int(year), month_to_int[l[0]], to_int(l[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wiki_table_to_df(end_url, month_range, base_url=base_url):\n",
    "    '''Creates a dataframe from the tables available in the wikipedia page'''\n",
    "    print('Scraping for', end_url)\n",
    "    r = requests.get(base_url + end_url) # Get request\n",
    "    soup = BeautifulSoup(r.text, 'lxml') # Parse HTML\n",
    "    wiki_tables = soup.findAll('table', {'class': 'wikitable sortable'}) # Get tables from the wikipedia page\n",
    "\n",
    "    table = []\n",
    "\n",
    "    for month_int, wiki_table in zip(month_range, wiki_tables):\n",
    "        for row in wiki_table.findAll('tr'):\n",
    "            elems = row.findAll('td') \n",
    "            if len(elems) != 0:\n",
    "                interesting = [elem.text for elem in elems[:5]]\n",
    "                 # First element is the day of the month, but we add the name of the month as well in front of it\n",
    "                interesting[0] = int_to_month[month_int] + ' ' + interesting[0]\n",
    "                table.append(interesting)\n",
    "                \n",
    "    df = pd.DataFrame(table, columns=['date', 'type', 'deaths', 'injuries', 'location'])\n",
    "    df.date = df.date.apply(lambda s: to_date(s, end_url[-4:])) # Translate the date with the year defined by the end_url arg\n",
    "    df.deaths = df.deaths.apply(to_int) # Map death number to int\n",
    "    df.injuries = df.injuries.apply(to_int) # Map injuries number to int\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping for July-December_2013\n"
     ]
    },
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-fd112f45a135>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Get a DataFrame for every article from 2011 to 2017\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonth_range\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtimes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwiki_table_to_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonth_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-53cb16edb657>\u001b[0m in \u001b[0;36mwiki_table_to_df\u001b[0;34m(end_url, month_range, base_url)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Scraping for'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_url\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mend_url\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Get request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lxml'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Parse HTML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mwiki_tables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'table'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'wikitable sortable'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Get tables from the wikipedia page\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/ada/lib/python3.5/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                     \u001b[0;34m\"Couldn't find a tree builder with the features you \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                     \u001b[0;34m\"requested: %s. Do you need to install a parser library?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m                     % \",\".join(features))\n\u001b[0m\u001b[1;32m    166\u001b[0m             \u001b[0mbuilder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuilder_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             if not (original_features == builder.NAME or\n",
      "\u001b[0;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "\n",
    "# Get a DataFrame for every article from 2011 to 2017\n",
    "for time, month_range in times.items():\n",
    "    dfs.append(wiki_table_to_df(time, month_range))\n",
    "    \n",
    "df = pd.concat(dfs)\n",
    "print('We have {} registered attacks from January 1st, 2011 up to today (November 28th, 2017)'.format(df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what some of the entries of the final result look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>date</th>\n",
       "      <th>type</th>\n",
       "      <th>deaths</th>\n",
       "      <th>injuries</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>Suicide bombing</td>\n",
       "      <td>21.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>Alexandria, Egypt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>56</td>\n",
       "      <td>2011-02-13</td>\n",
       "      <td>Raid</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Zamboanga, Philippines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1033</th>\n",
       "      <td>37</td>\n",
       "      <td>2014-11-18</td>\n",
       "      <td>Shooting, Melee attack</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Jerusalem, Israel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4663</th>\n",
       "      <td>41</td>\n",
       "      <td>2017-11-28</td>\n",
       "      <td>Bombing</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kandahar province, Afghanistan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index        date                    type  deaths  injuries  \\\n",
       "0         0  2011-01-01         Suicide bombing    21.0      97.0   \n",
       "56       56  2011-02-13                    Raid     7.0       5.0   \n",
       "1033     37  2014-11-18  Shooting, Melee attack     5.0       7.0   \n",
       "4663     41  2017-11-28                 Bombing     8.0       NaN   \n",
       "\n",
       "                            location  \n",
       "0                  Alexandria, Egypt  \n",
       "56            Zamboanga, Philippines  \n",
       "1033               Jerusalem, Israel  \n",
       "4663  Kandahar province, Afghanistan  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[[0, 56, 1033, -1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reindex and save\n",
    "df.to_csv('attacks.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Making sense of the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INFO SUR LA DATA : 548847 raw Tweets, env. 350'000 sans les RT, s'arrete le 31 Jan 2016\n",
    "\n",
    "- reussir a importer dans dataframe\n",
    "- enlever wiki data apres dernier tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Create new DataFrame with Tweets and Wiki data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create DF : **date, attack type, city, country, real impact, deads, injured, social impact, number of tweets**\n",
    "\n",
    "- to merge both : match with date of event, and maybe a function that gives a matching score (name of town, country)\n",
    "- take into account the tweets from the date to a certain amount of days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Plot the attacks in a map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- highlight the real impacts and the social impact\n",
    "- folium ? ideally, map with circles for real impact, and (also circle ?) for social impacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Other graphs/info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- all the #prayfor : list the different towns\n",
    "- number of attacks/deaths by country (only taken from **Wikipedia dataset**)\n",
    "- ranking of the most liked/ignored attack (just divide social impact by real impact)\n",
    "- same but with country (aggregate social and total impact from before (more relevant than previous point)\n",
    "- maybe see the rise of ISIS ? find by keyword (ISIS) and check with the timeline (graph ISIS claimed vs time, **Wikipedia dataset**)\n",
    "- finally, see the fading of reactions over time \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2 Safety ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec la partie 3.2 et 3.3, on devrait avoir assez d'info a leur montrer ! \n",
    "\n",
    "Et si on galère niveau temps, on se replie sur displayer des trucs sur le dataset wikipedia !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Do the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
