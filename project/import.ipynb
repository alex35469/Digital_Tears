{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADA Project : Milestone 2 Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will introduce you to the dataset that we chose by locally importing a part of in, and store it in a dataframe. Hence, we will be able to have an insight on the work that we will perform on the full dataset, once everything is set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from pyspark.sql import *\n",
    "from pyspark import SparkContext, SQLContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Twitter dataset data collection, from cluster to dataframe\n",
    "\n",
    "In this section, we will make some operation with the help of Spark, to access, filter and export the useful tweets from the cluster to our computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few words about what we noticed for our dataset \n",
    "\n",
    "First, the twitter dataset starts from year 2012.\n",
    "In the date section, the hour has been scaled, so that the tweet time is always relative to GMT+00. This will be of use when we will relate tweet dates and times with the Wikipedia dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i) filtering the useful tweets\n",
    "\n",
    "We start by declaring the Spark Context in order to make the link with the cluster. With Spark installed locally, we are able to query the cluster directly in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-2-4443dd5159b0>:1 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-4443dd5159b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/envs/ada/lib/python3.5/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m~/anaconda/envs/ada/lib/python3.5/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    297\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 299\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-2-4443dd5159b0>:1 "
     ]
    }
   ],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_file = sc.textFile(\"hdfs:///datasets/tweets-leon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of our filter is that we want to work with data that is already highly focused on our subject : terrorist  attacks. Moreover, to avoid some bias that could appear from the selection of a subset of the entire Twitter database, we will consider all the most common languages, and select the interesting tweets independently of the language.\n",
    "Indeed, the filter is composed of lists of words of different weight, for each language. When a tweet is passed through the filter, we will compute the tweet score depending on it's content and, if the score is high enough, select the tweet to be part of our dataframe. Since all selected words are translated, this should harmonize the selection of our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We define below a few helper functions that will be used for our inition filter :** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first helper function is used to translate a word of interest in every selected language :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_word(w,languages):\n",
    "    \n",
    "    translator = Translator()\n",
    "\n",
    "\n",
    "    w_dict = {}\n",
    "    \n",
    "    for l in languages:\n",
    "        t = translator.translate(w,dest=l)\n",
    "        w_dict[l]=t.text\n",
    "        \n",
    "    \n",
    "    \n",
    "    return w_dict\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is the heart of the filter. Languages and words are selected and inputed directly into three different lists of different weight. They are translated and put into three dicts, the keys being the language and the values being all words of the selected language, from the selected \"importance list\".\n",
    "The function outputs the three dicts, one for each importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_match():\n",
    "    \n",
    "    big_dict1 = {}\n",
    "    big_dict2 = {}\n",
    "    big_dict3 = {}\n",
    "    languages = ['en','fr','es','it','nl','de']\n",
    "    \n",
    "    for l in languages:\n",
    "        big_dict1[l] = []\n",
    "        big_dict2[l] = []\n",
    "        big_dict3[l] = []\n",
    "    \n",
    "    interesting_words_t1 = ['terrorism', 'terrorist', 'terror attack']\n",
    "    for w in interesting_words_t1:\n",
    "        d = translate_word(w,languages)\n",
    "        for (k,v) in d.items():\n",
    "            big_dict1[k].append(v)\n",
    "\n",
    "    interesting_words_t2 = ['isis', 'bomb','hostage']\n",
    "    for w in interesting_words_t2:\n",
    "        d = translate_word(w,languages)\n",
    "        for (k,v) in d.items():\n",
    "            big_dict2[k].append(v)\n",
    "\n",
    "\n",
    "    interesting_words_t3 = ['weapon','knife','assault rifle','dead','deaths','died','injured','kill','attentat']\n",
    "    for w in interesting_words_t3:\n",
    "        d = translate_word(w,languages)\n",
    "        for (k,v) in d.items():\n",
    "            big_dict3[k].append(v)\n",
    "        \n",
    "    #list_special_words = []\n",
    "    \n",
    "\n",
    "    return (big_dict1,big_dict2,big_dict3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function computes the importance of a tweet by assigning specific weights to every tweet. The assignment is done by iterating on all interesting words, looking whether they occur in the tweet content. According to the word's affiliation to one of the three dicts, different weight is incremented. If the total weight of the tweet reaches the threshold value (here 1.0), the filter returns True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_interesting(content,big_dicts):\n",
    "    \n",
    "\n",
    "    content = parse_tweet_content(content)\n",
    "    \n",
    "    languages = ['en','fr','es','it','nl','de'] \n",
    "\n",
    "    lang = content[:2]\n",
    "    \n",
    "    # Here we are concentrating our analysis in a limited number of languages\n",
    "    if lang not in languages:\n",
    "        return False\n",
    "    \n",
    "    #TODO\n",
    "    #no specific place, specific religion, specific ethnic group etc.\n",
    "    #exception : big terrorist groups ?\n",
    "    #inclure filtre ville\n",
    "    \n",
    "    weight=0.0\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for w in big_dicts[0][lang]:\n",
    "        if w in content:\n",
    "            weight+=1.0\n",
    "\n",
    "    for w in big_dicts[1][lang]:\n",
    "        if w in content:\n",
    "            weight+=0.5\n",
    "\n",
    "    for w in big_dicts[2][lang]:\n",
    "        if w in content:\n",
    "            weight+=0.3\n",
    "             \n",
    "    return (weight >= 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We declare the variable `bds` to be the three filtering dictionnaries. It will serve as an input of our filtering function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bds = words_to_match()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we actually call spark by filtering the data in the cluster with our filter, to then take a subset of defined size. We proceed to write it to a text file for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "terrorism = text_file.filter(lambda t: is_interesting(t,bds)).take(4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_t = open('tweets_terror2.txt','w')\n",
    "for item in terrorism:\n",
    "    file_t.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an important part of our analysis, we want to have a feel on the percentage of the tweets that we filter. According to our filter's characteristics, the ratio of tweets of interest over all tweets is of 1/3000. Taking a total tweet count of around 18 billion, the total tweets that are interesting for us is of 6 million."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terr = text_file.take(300000)\n",
    "\n",
    "count=0\n",
    "for t in terr:\n",
    "    if is_interesting(t,bds):\n",
    "        count+=1\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that we get almost 100 matches for a subset of 300'000 tweets, illustrating the ratio of 1/3000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fr\\t345963978092072960\\tSat Jun 15 18:00:14 +0000 2013\\tKazdaliMaaradj\\tPakistan: un double-attentat à la bombe à Quetta (sud-ouest) fait au moins 23 morts (nouveau bilan des autorités locales)',\n",
       " 'en\\t345964011382259712\\tSat Jun 15 18:00:22 +0000 2013\\tSumairaALi4\\t#BLA needs to be targetted in INDIA and UK. ISI should get in motion as were in 80s #JudicialTerrorism',\n",
       " 'en\\t345964045007978497\\tSat Jun 15 18:00:30 +0000 2013\\tMonotheist_\\tUnrest in Baluchistan. BLA terrorism there. Baluch demand justice. Foreign and local intelligence are involved. No one dares to anyone',\n",
       " 'es\\t345964057632837632\\tSat Jun 15 18:00:33 +0000 2013\\texodo3013\\t@akatsuky1000 quien ayudo a librar al pueblo de Libia del terrorista #1 en el mundo Omar K  que masacraba a su pueblo con aviones de guerra',\n",
       " 'es\\t345964070274482176\\tSat Jun 15 18:00:36 +0000 2013\\tCesar_Soto_16\\tCon Los Terroristas - Alianza Metal 1°H: http://t.co/SDT9qzSVHz vía @YouTube']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terrorism[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada]",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
